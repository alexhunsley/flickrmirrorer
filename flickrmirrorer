#!/usr/bin/env python

# A small command-line python script that creates a local backup of your
# Flickr data.  It mirrors images, titles, description, tags, sets and
# collections.
#
# Available at https://github.com/markdoliner/flickrmirrorer
#
# Licensed as follows (this is the 2-clause BSD license, aka
# "Simplified BSD License" or "FreeBSD License"):
#
# Copyright (c) 2012-2013, Mark Doliner
#               2013, Mattias Holmlund
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
# - Redistributions of source code must retain the above copyright notice,
#   this list of conditions and the following disclaimer.
# - Redistributions in binary form must reproduce the above copyright notice,
#   this list of conditions and the following disclaimer in the documentation
#   and/or other materials provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.

import argparse
import errno
import math
import os
import shutil
import sys
import urllib

try:
    # We try importing simplejson first because it's faster than json
    # in python 2.7 and lower
    import simplejson as json
except ImportError:
    import json

try:
    import flickrapi
except ImportError:
    sys.stderr.write('Error importing flickrapi python library.  Is it installed?\n')
    sys.exit(1)

API_KEY = '9c5c431017e712bde232a2f142703bb2'
API_SECRET = '7c024f6e7a36fc03'

PLEASE_GRANT_AUTHORIZATION_MSG = """
Please authorize Flickr Mirrorer to read your photos, titles, tags, etc.
Press 'return' when finished.

"""

NUM_PHOTOS_PER_BATCH = 500

def ensure_dir_exists(path):
    """Create the directory 'path' if it does not exist.
    Calls sys.exit(1) if any directory could not be created."""
    try:
        os.makedirs(path)
    except OSError, e:
        if e.errno != errno.EEXIST:
            sys.stderr.write('Error creating destination directory %s: %s\n'
                    % (path, e.strerror))
            sys.exit(1)

def ensure_dir_doesnt_exist(path):
    """Remove the directory 'path' and all contents if it exists.
    Calls sys.exit(1) if the directory or any contents could not be removed."""
    try:
        shutil.rmtree(path)
    except OSError, e:
        if e.errno != errno.ENOENT:
            sys.stderr.write('Error removing %s: %s\n' % (path, e.strerror))
            sys.exit(1)

# flickrapi should really do this for us :-(
def parse_json_response(rsp):
    """Parse the flickr API response as JSON."""
    # Strip off the "jsonFlickrApi({...});" wrapper
    rsp = rsp[14:-1]
    rsp = json.loads(rsp)
    if rsp['stat'] != 'ok':
        sys.stderr.write('API request failed: Error %(code)s: %(message)s\n' % rsp)
        sys.exit(1)
    return rsp

class FlickrMirrorer:
    dest_dir = None
    photostream_dir = None
    tmp_filename = None
    flickr = None

    def __init__(self, dest_dir, verbosity, print_statistics, include_views):
        self.dest_dir = dest_dir
        self.verbosity = verbosity
        self.print_statistics = print_statistics
        self.include_views = include_views
        self.photostream_dir = os.path.join(self.dest_dir, 'photostream')
        self.tmp_filename = os.path.join(self.dest_dir, 'tmp')

        # Statistics
        self.deleted_photos = 0
        self.modified_photos = 0
        self.new_photos = 0
        self.modified_sets = 0

        # Create flickrapi instance
        self.flickr = flickrapi.FlickrAPI(API_KEY, secret=API_SECRET, format='json')

    def run(self):
        try:
            self._run_helper()
        finally:
            self._cleanup()

    def _run_helper(self):
        # Authenticate
        (token, frob) = self.flickr.get_token_part_one(perms='read')
        if not token:
            raw_input(PLEASE_GRANT_AUTHORIZATION_MSG)
        try:
            self.flickr.get_token_part_two((token, frob))
        except flickrapi.exceptions.FlickrError, e:
            sys.stderr.write('Error authenticating: %s\n' % e.message)
            sys.exit(1)

        # Create destination directory
        ensure_dir_exists(self.dest_dir)

        # Fetch photos
        self._download_all_photos()

        # Create sets and collections
        self._mirror_photosets()
        self._create_not_in_photoset_dir()
        self._mirror_collections()

        if self.print_statistics:
            print "New photos: %d" % self.new_photos
            print "Deleted photos: %d" % self.deleted_photos
            print "Modified photos: %d" % self.modified_photos
            print "Modified sets: %d" % self.modified_sets

    def _download_all_photos(self):
        """Download all our pictures and metadata.
        If you have a lot of photos then this function will take a while."""

        self._verbose('Fetching all photos from photostream')

        ensure_dir_exists(self.photostream_dir)

        old_files = os.listdir(self.photostream_dir)
        new_files = set()

        current_page = 1

        metadata_fields = 'description,license,date_upload,date_taken,owner_name,icon_server,original_format,last_update,geo,tags,machine_tags,o_dims,media'

        if self.include_views:
            metadata_fields += ',views'

        while True:
            rsp = self.flickr.people_getPhotos(
                    user_id='me',
                    extras=metadata_fields,
                    per_page=NUM_PHOTOS_PER_BATCH,
                    page=current_page,
            )
            rsp = parse_json_response(rsp)

            photos = rsp['photos']['photo']
            if not photos:
                # We've reached the end of the photostream.  Stop looping.
                break
            for photo in photos:
                new_files |= self._download_photo(photo)

            current_page += 1

        unknown_files = set(old_files) - set(new_files)
        for unknown_file in unknown_files:
            unknown_filename = os.path.join(self.photostream_dir, unknown_file)
            self._progress('Deleting unknown file: %s' % unknown_filename)
            self.deleted_photos += 1
            try:
                os.remove(unknown_filename)
            except OSError, e:
                sys.stderr.write('Error deleting %s: %s\n' % (unknown_filename, e.strerror))
                sys.exit(1)

    def _download_photo(self, photo):
        """Fetch and save a photo and the metadata for the photo.

        Returns a set containing the filenames for the data."""
        url = 'http://farm%(farm)s.staticflickr.com/%(server)s/%(id)s_%(originalsecret)s_o.%(originalformat)s' % photo
        photo_basename = '%s.%s' % (photo['id'], photo['originalformat'])
        photo_filename = os.path.join(self.photostream_dir, photo_basename)
        metadata_basename = '%s.metadata' % photo_basename
        metadata_filename = '%s.metadata' % photo_filename

        # Sanity check
        if os.path.isdir(photo_filename) or os.path.islink(photo_filename):
            sys.stderr.write('Error: %s exists but is not a file.  This is not allowed.\n' % photo_filename)
            sys.exit(1)

        # Sanity check
        if os.path.isdir(metadata_filename) or os.path.islink(metadata_filename):
            sys.stderr.write('Error: %s exists but is not a file.  This is not allowed.\n' % metadata_filename)
            sys.exit(1)

        # Check if we should fetch the image
        if not os.path.exists(photo_filename) \
                or int(photo['lastupdate']) >= os.lstat(photo_filename).st_mtime:
            # We don't have this photo or the version on the server is newer

            if not os.path.exists(photo_filename):
                self.new_photos += 1
            else:
                self.modified_photos += 1

            self._progress('Fetching %s' % photo_basename)
            data = urllib.urlretrieve(url, self.tmp_filename)
            os.rename(self.tmp_filename, photo_filename)
        else:
            self._verbose('Skipping %s because we already have it'
                          % photo_basename)

        # Write metadata
        if self._write_json_if_changed(metadata_filename, photo):
            self._progress('Updated metadata for %s' % photo_basename)
        else:
            self._verbose(
                'Skipping metadata for %s because we already have it' %
                photo_basename )

        return {photo_basename, metadata_basename}

    def _mirror_photosets(self):
        """Create a directory for each photoset, and create symlinks to the
        files in the photostream."""
        self._verbose('Creating local sets...')

        # Fetch photosets
        rsp = self.flickr.photosets_getList()
        rsp = parse_json_response(rsp)
        if rsp['photosets']:
            for photoset in rsp['photosets']['photoset']:
                self._mirror_photoset(photoset)

    def _mirror_photoset(self, photoset):
        photoset_basename = self._get_set_dirname(photoset['id'], photoset['title']['_content'])
        photoset_dir = os.path.join(self.dest_dir, photoset_basename)

        # Fetch list of photos
        photoids = []
        originalformat = {}

        num_pages = int(math.ceil(float(photoset['photos']) / NUM_PHOTOS_PER_BATCH))
        for current_page in range(1, num_pages + 1):
            # Fetch photos in this photoset
            rsp = self.flickr.photosets_getPhotos(
                    photoset_id=photoset['id'],
                    extras='original_format',
                    per_page=NUM_PHOTOS_PER_BATCH,
                    page=current_page,
            )
            rsp = parse_json_response(rsp)

            for photo in rsp['photoset']['photo']:
                photoids.append(photo['id'])
                originalformat[photo['id']] = photo['originalformat']

        # Include list of pictures in metadata
        photoset['photos'] = photoids

        if (not self.include_views) and 'count_views' in photoset:
            del photoset['count_views']

        ensure_dir_exists(photoset_dir)

        # Write metadata
        metadata_filename = os.path.join(photoset_dir, 'metadata')

        if self._write_json_if_changed(metadata_filename, photoset):
            # Metadata changed, might be due to updated list of photos.
            # Recreate all symlinks
            self._progress('Updating set %s' % photoset['title']['_content'])
            self.modified_sets += 1

            ensure_dir_doesnt_exist(photoset_dir)
            ensure_dir_exists(photoset_dir)
            self._write_json_if_changed(metadata_filename, photoset)

            for id in photoids:
                photo_basename = '%s.%s' % (id, originalformat[id])
                photo_filename = os.path.join('..', 'photostream', photo_basename)
                symlink_filename = os.path.join(photoset_dir, photo_basename)
                os.symlink(photo_filename, symlink_filename)

        else:
            self._verbose('Set %s is up-to-date' % photoset['title']['_content'])


    def _create_not_in_photoset_dir(self):
        """Create a directory for photos that aren't in any photoset, and
        create symlinks to the files in the photostream."""

        self._verbose('Creating local directory for photos not in a set')

        photoset_dir = os.path.join(self.dest_dir, 'Not in any set')

        # TODO: Ideally we would inspect the existing directory and
        # make sure it's correct, but that's a lot of work. For now
        # just recreate the set. Fixing this would also allow us to
        # log _progress() messages when the set has changed.
        ensure_dir_doesnt_exist(photoset_dir)
        ensure_dir_exists(photoset_dir)

        # Fetch list of photos
        current_page = 1
        while True:
            # Fetch photos in this photoset
            rsp = self.flickr.photos_getNotInSet(
                    extras='original_format',
                    per_page=NUM_PHOTOS_PER_BATCH,
                    page=current_page,
            )
            rsp = parse_json_response(rsp)
            photos = rsp['photos']['photo']
            if not photos:
                # We've reached the end of the photostream.  Stop looping.
                break

            for photo in photos:
                photo_basename = '%s.%s' % (photo['id'], photo['originalformat'])
                photo_filename = os.path.join('..', 'photostream', photo_basename)
                symlink_filename = os.path.join(photoset_dir, photo_basename)
                os.symlink(photo_filename, symlink_filename)

            current_page += 1

    def _mirror_collections(self):
        """Create a directory for each collection, and create symlinks to the
        sets."""
        self._verbose('Creating local collections...')

        # Fetch collections
        rsp = self.flickr.collections_getTree()
        rsp = parse_json_response(rsp)
        if rsp['collections']:
            for collection in rsp['collections']['collection']:
                self._mirror_collection(collection)

    def _mirror_collection(self, collection):
        self._verbose('Creating local collection %s' % collection['title'])

        collection_basename = self._get_collection_dirname(collection['id'], collection['title'])
        collection_dir = os.path.join(self.dest_dir, collection_basename)

        # TODO: Ideally we would inspect the existing directory and
        # make sure it's correct, but that's a lot of work. For now
        # just recreate the collection. Fixing this would also allow
        # us to log _progress() messages when the collection has
        # changed.
        ensure_dir_doesnt_exist(collection_dir)
        ensure_dir_exists(collection_dir)

        # Write metadata
        metadata_filename = os.path.join(collection_dir, 'metadata')
        self._write_json_if_changed(metadata_filename, collection)

        for set_ in collection['set']:
            set_basename = self._get_set_dirname(set_['id'], set_['title'])
            set_filename = os.path.join('..', set_basename)
            symlink_filename = os.path.join(collection_dir, set_basename)
            os.symlink(set_filename, symlink_filename)

    def _get_set_dirname(self, id_, title):
        safe_title = urllib.quote(title.encode('utf-8'), " ',")
        # TODO: We use the ID in the name to avoid conflicts when there are
        #       two sets with the same name.  Is there a better way to
        #       handle that?  Maybe by using the date of the oldest picture
        #       instead?
        return 'Set %s - %s' % (id_, safe_title)

    def _get_collection_dirname(self, id_, title):
        safe_title = urllib.quote(title.encode('utf-8'), " ',")
        # TODO: We use the ID in the name to avoid conflicts when there are
        #       two collections with the same name (is that even possible?)
        #       Is there a better way to handle that?
        return 'Collection %s - %s' % (id_, safe_title)

    # Using this function isn't really necessary, but it's nice to avoid
    # unnecessarily changing the timestamps on metadata files.
    def _write_json_if_changed(self, filename, data):
        """Write the given data to the specified filename, but only if it's
        different from what is currently there. Return true if the file was
        written."""
        try:
            orig_data = json.load(open(filename))
            if orig_data == data:
                # Data has not changed--do nothing.
                return False
        except IOError, e:
            if e.errno != errno.ENOENT:
                sys.stderr.write('Error reading %s: %s\n' % (filename, e))
                sys.exit(1)

        f = open(self.tmp_filename, 'w')
        json.dump(data, f)
        f.close()
        os.rename(self.tmp_filename, filename)
        return True

    def _verbose(self, msg):
        if self.verbosity >= 2:
            print msg

    def _progress(self, msg):
        if self.verbosity >= 1:
            print msg

    def _cleanup(self):
        # Remove a temp file, if one exists
        try:
            os.remove(self.tmp_filename)
        except OSError, e:
            if e.errno != errno.ENOENT:
                sys.stderr.write('Error deleting temp file %s: %s\n' % (self.tmp_filename, e))

if __name__ == '__main__':
    try:
        parser = argparse.ArgumentParser(
            description='Create a local mirror of your flickr data.')

        parser.add_argument(
            'destdir',
            help='the path to where the mirror shall be stored')

        parser.add_argument(
            '-v', '--verbose',
            dest='verbosity', action="store_const", const=2,
            default=1,
            help='print progress information to stdout')

        parser.add_argument(
            '-q', '--quiet',
            dest='verbosity', action="store_const", const=0,
            help='print nothing to stdout if the mirror succeeds')

        parser.add_argument(
            '-s', '--statistics', action="store_const",
            default=False, const=True,
            help='print transfer-statistics at the end')

        parser.add_argument(
            '--ignore-views', action="store_const",
            dest='include_views', default=True, const=False,
            help="do not include views-counter in metadata" )

        args = parser.parse_args()

        mirrorer = FlickrMirrorer(args.destdir, args.verbosity,
                                  args.statistics, args.include_views)
        mirrorer.run()
    except KeyboardInterrupt:
        # User exited with CTRL+C
        # Print a newline to leave the console in a prettier state
        print
