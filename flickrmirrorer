#!/usr/bin/env python

# A small command-line python script that creates a local backup of your
# Flickr data.  It mirrors images, titles, description, tags, sets and
# collections.
#
# Available at https://github.com/markdoliner/flickrmirrorer
#
# Licensed as follows (this is the 2-clause BSD license, aka
# "Simplified BSD License" or "FreeBSD License"):
#
# Copyright (c) 2012-2013, Mark Doliner
#               2013, Mattias Holmlund
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
# - Redistributions of source code must retain the above copyright notice,
#   this list of conditions and the following disclaimer.
# - Redistributions in binary form must reproduce the above copyright notice,
#   this list of conditions and the following disclaimer in the documentation
#   and/or other materials provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.

import argparse
import errno
import math
import os
import shutil
import sys
import urllib

try:
    # We try importing simplejson first because it's faster than json
    # in python 2.7 and lower
    import simplejson as json
except ImportError:
    import json

try:
    import flickrapi
except ImportError:
    sys.stderr.write('Error importing flickrapi python library.  Is it installed?\n')
    sys.exit(1)

API_KEY = '9c5c431017e712bde232a2f142703bb2'
API_SECRET = '7c024f6e7a36fc03'

PLEASE_GRANT_AUTHORIZATION_MSG = """
Please authorize Flickr Mirrorer to read your photos, titles, tags, etc.
Press 'return' when finished.

"""

NUM_PHOTOS_PER_BATCH = 500

def ensure_dir_exists(path):
    """Create the directory 'path' if it does not exist.
    Calls sys.exit(1) if any directory could not be created."""
    try:
        os.makedirs(path)
    except OSError, e:
        if e.errno != errno.EEXIST:
            sys.stderr.write('Error creating destination directory %s: %s\n'
                    % (path, e.strerror))
            sys.exit(1)

def ensure_dir_doesnt_exist(path):
    """Remove the directory 'path' and all contents if it exists.
    Calls sys.exit(1) if the directory or any contents could not be removed."""
    try:
        shutil.rmtree(path)
    except OSError, e:
        if e.errno != errno.ENOENT:
            sys.stderr.write('Error removing %s: %s\n' % (path, e.strerror))
            sys.exit(1)

# flickrapi should really do this for us :-(
def parse_json_response(rsp):
    """Parse the flickr API response as JSON."""
    # Strip off the "jsonFlickrApi({...});" wrapper
    rsp = rsp[14:-1]
    rsp = json.loads(rsp)
    if rsp['stat'] != 'ok':
        sys.stderr.write('API request failed: Error %(code)s: %(message)s\n' % rsp)
        sys.exit(1)
    return rsp

class FlickrMirrorer:
    dest_dir = None
    photostream_dir = None
    tmp_filename = None
    flickr = None

    def __init__(self, dest_dir, verbosity, print_statistics, include_views):
        self.dest_dir = dest_dir
        self.verbosity = verbosity
        self.print_statistics = print_statistics
        self.include_views = include_views
        self.photostream_dir = os.path.join(self.dest_dir, 'photostream')
        self.sets_dir = os.path.join(self.dest_dir, 'Sets')
        self.collections_dir = os.path.join(self.dest_dir, 'Collections')
        self.tmp_filename = os.path.join(self.dest_dir, 'tmp')

        # Statistics
        self.deleted_photos = 0
        self.modified_photos = 0
        self.new_photos = 0
        self.modified_sets = 0
        self.modified_collections = 0

        # Create flickrapi instance
        self.flickr = flickrapi.FlickrAPI(API_KEY, secret=API_SECRET, format='json')

    def run(self):
        try:
            self._run_helper()
        finally:
            self._cleanup()

    def _run_helper(self):
        # Authenticate
        (token, frob) = self.flickr.get_token_part_one(perms='read')
        if not token:
            raw_input(PLEASE_GRANT_AUTHORIZATION_MSG)
        try:
            self.flickr.get_token_part_two((token, frob))
        except flickrapi.exceptions.FlickrError, e:
            sys.stderr.write('Error authenticating: %s\n' % e.message)
            sys.exit(1)

        # Create destination directory
        ensure_dir_exists(self.dest_dir)

        # Fetch photos
        self._download_all_photos()

        # Create sets and collections
        self._mirror_photosets()
        self._create_not_in_photoset_dir()
        self._mirror_collections()

        if self.print_statistics:
            print "New photos: %d" % self.new_photos
            print "Deleted photos: %d" % self.deleted_photos
            print "Modified photos: %d" % self.modified_photos
            print "Modified sets: %d" % self.modified_sets
            print "Modified collections: %d" % self.modified_collections

    def _download_all_photos(self):
        """Download all our pictures and metadata.
        If you have a lot of photos then this function will take a while."""

        self._verbose('Fetching all photos from photostream')

        ensure_dir_exists(self.photostream_dir)

        new_files = set()

        current_page = 1

        metadata_fields = 'description,license,date_upload,date_taken,owner_name,icon_server,original_format,last_update,geo,tags,machine_tags,o_dims,media'

        if self.include_views:
            metadata_fields += ',views'

        while True:
            rsp = self.flickr.people_getPhotos(
                    user_id='me',
                    extras=metadata_fields,
                    per_page=NUM_PHOTOS_PER_BATCH,
                    page=current_page,
            )
            rsp = parse_json_response(rsp)

            photos = rsp['photos']['photo']
            if not photos:
                # We've reached the end of the photostream.  Stop looping.
                break
            for photo in photos:
                new_files |= self._download_photo(photo)

            current_page += 1

        # Divide by 2 because we want to ignore the photo metadata files
        # for the purposes of our statistics.
        self.deleted_photos = self._delete_unknown_files(self.photostream_dir, new_files, "file") / 2

    def _download_photo(self, photo):
        """Fetch and save a photo and the metadata for the photo.

        Returns a set containing the filenames for the data."""
        url = 'http://farm%(farm)s.staticflickr.com/%(server)s/%(id)s_%(originalsecret)s_o.%(originalformat)s' % photo
        photo_basename = '%s.%s' % (photo['id'], photo['originalformat'])
        photo_filename = os.path.join(self.photostream_dir, photo_basename)
        metadata_basename = '%s.metadata' % photo_basename
        metadata_filename = '%s.metadata' % photo_filename

        # Sanity check
        if os.path.isdir(photo_filename) or os.path.islink(photo_filename):
            sys.stderr.write('Error: %s exists but is not a file.  This is not allowed.\n' % photo_filename)
            sys.exit(1)

        # Sanity check
        if os.path.isdir(metadata_filename) or os.path.islink(metadata_filename):
            sys.stderr.write('Error: %s exists but is not a file.  This is not allowed.\n' % metadata_filename)
            sys.exit(1)

        # Check if we should fetch the image
        if not os.path.exists(photo_filename) \
                or int(photo['lastupdate']) >= os.lstat(photo_filename).st_mtime:
            # We don't have this photo or the version on the server is newer

            if not os.path.exists(photo_filename):
                self.new_photos += 1
            else:
                self.modified_photos += 1

            self._progress('Fetching %s' % photo_basename)
            data = urllib.urlretrieve(url, self.tmp_filename)
            os.rename(self.tmp_filename, photo_filename)
        else:
            self._verbose('Skipping %s because we already have it'
                          % photo_basename)

        # Write metadata
        if self._write_json_if_changed(metadata_filename, photo):
            self._progress('Updated metadata for %s' % photo_basename)
        else:
            self._verbose(
                'Skipping metadata for %s because we already have it' %
                photo_basename )

        return {photo_basename, metadata_basename}

    def _mirror_photosets(self):
        """Create a directory for each photoset, and create symlinks to the
        files in the photostream."""
        self._verbose('Creating local sets...')

        photoset_dirs = set()

        # Fetch photosets
        rsp = self.flickr.photosets_getList()
        rsp = parse_json_response(rsp)
        if rsp['photosets']:
            for photoset in rsp['photosets']['photoset']:
                photoset_dirs |= self._mirror_photoset(photoset)

        self._delete_unknown_files(self.sets_dir, photoset_dirs, "set")

    def _mirror_photoset(self, photoset):
        photoset_basename = self._get_set_dirname(photoset['id'], photoset['title']['_content'])
        photoset_dir = os.path.join(self.sets_dir, photoset_basename)

        # Fetch list of photos
        photoids = []
        originalformat = {}

        num_pages = int(math.ceil(float(photoset['photos']) / NUM_PHOTOS_PER_BATCH))
        for current_page in range(1, num_pages + 1):
            # Fetch photos in this photoset
            rsp = self.flickr.photosets_getPhotos(
                    photoset_id=photoset['id'],
                    extras='original_format',
                    per_page=NUM_PHOTOS_PER_BATCH,
                    page=current_page,
            )
            rsp = parse_json_response(rsp)

            for photo in rsp['photoset']['photo']:
                photoids.append(photo['id'])
                originalformat[photo['id']] = photo['originalformat']

        # Include list of pictures in metadata
        photoset['photos'] = photoids

        if (not self.include_views) and 'count_views' in photoset:
            del photoset['count_views']

        metadata_filename = os.path.join(photoset_dir, 'metadata')

        if not os.path.exists(photoset_dir) or self._is_file_different(metadata_filename, photoset):
            # Metadata changed, might be due to updated list of photos.
            self._progress('Updating set %s' % photoset['title']['_content'])
            self.modified_sets += 1

            # Delete and recreate the set
            ensure_dir_doesnt_exist(photoset_dir)
            ensure_dir_exists(photoset_dir)

            # Create symlinks for each photo, prefixed with a number so that
            # the local alphanumeric sort order matches the order on Flickr.
            digits = len(str(len(photoids)))
            for i,id in enumerate(photoids):
                photo_basename = '%s.%s' % (id, originalformat[id])
                photo_fullname = os.path.join(self.photostream_dir, photo_basename)
                photo_relname = os.path.relpath(photo_fullname, photoset_dir)
                symlink_basename = '%s_%s.%s' % (str(i+1).zfill(digits), id, originalformat[id])
                symlink_filename = os.path.join(photoset_dir, symlink_basename)
                os.symlink(photo_relname, symlink_filename)

            # Write metadata
            self._write_json_if_changed(metadata_filename, photoset)

        else:
            self._verbose('Set %s is up-to-date' % photoset['title']['_content'])

        return {photoset_basename}

    def _create_not_in_photoset_dir(self):
        """Create a directory for photos that aren't in any photoset, and
        create symlinks to the files in the photostream."""

        self._verbose('Creating local directory for photos not in a set')

        photoset_dir = os.path.join(self.dest_dir, 'Not in any set')

        # TODO: Ideally we would inspect the existing directory and
        # make sure it's correct, but that's a lot of work. For now
        # just recreate the set. Fixing this would also allow us to
        # log _progress() messages when the set has changed.
        ensure_dir_doesnt_exist(photoset_dir)
        ensure_dir_exists(photoset_dir)

        # Fetch list of photos
        current_page = 1
        while True:
            # Fetch photos in this photoset
            rsp = self.flickr.photos_getNotInSet(
                    extras='original_format',
                    per_page=NUM_PHOTOS_PER_BATCH,
                    page=current_page,
            )
            rsp = parse_json_response(rsp)
            photos = rsp['photos']['photo']
            if not photos:
                # We've reached the end of the photostream.  Stop looping.
                break

            for photo in photos:
                photo_basename = '%s.%s' % (photo['id'], photo['originalformat'])
                photo_fullname = os.path.join(self.photostream_dir, photo_basename)
                photo_relname = os.path.relpath(photo_fullname, photoset_dir)
                symlink_filename = os.path.join(photoset_dir, photo_basename)
                os.symlink(photo_relname, symlink_filename)

            current_page += 1

    def _mirror_collections(self):
        """Create a directory for each collection, and create symlinks to the
        sets."""
        self._verbose('Creating local collections...')

        collection_dirs = set()

        # Fetch collections
        rsp = self.flickr.collections_getTree()
        rsp = parse_json_response(rsp)
        if rsp['collections']:
            for collection in rsp['collections']['collection']:
                collection_dirs = self._mirror_collection(collection)

        self._delete_unknown_files(self.collections_dir, collection_dirs, "collection")


    def _mirror_collection(self, collection):
        collection_basename = self._get_collection_dirname(collection['id'], collection['title'])
        collection_dir = os.path.join(self.collections_dir, collection_basename)

        metadata_filename = os.path.join(collection_dir, 'metadata')

        if not os.path.exists(collection_dir) or self._is_file_different(metadata_filename, collection):
            # Metadata changed, might be due to updated list of sets.
            self._progress('Updating collection %s' % collection['title'])
            self.modified_collections += 1

            # Delete and recreate the collection
            ensure_dir_doesnt_exist(collection_dir)
            ensure_dir_exists(collection_dir)

            # Create symlinks for each set
            for set_ in collection['set']:
                set_basename = self._get_set_dirname(set_['id'], set_['title'])
                set_fullname = os.path.join(self.sets_dir, set_basename)
                set_relname = os.path.relpath(set_fullname, collection_dir)
                symlink_filename = os.path.join(collection_dir, set_basename)
                os.symlink(set_relname, symlink_filename)

            # Write metadata
            self._write_json_if_changed(metadata_filename, collection)

        return {collection_basename}

    def _get_set_dirname(self, id_, title):
        safe_title = urllib.quote(title.encode('utf-8'), " ',")
        # TODO: We use the ID in the name to avoid conflicts when there are
        #       two sets with the same name.  Is there a better way to
        #       handle that?  Maybe by using the date of the oldest picture
        #       instead?
        return '%s - %s' % (safe_title, id_)

    def _get_collection_dirname(self, id_, title):
        safe_title = urllib.quote(title.encode('utf-8'), " ',")
        # TODO: We use the ID in the name to avoid conflicts when there are
        #       two collections with the same name (is that even possible?)
        #       Is there a better way to handle that?
        return '%s - %s' % (safe_title, id_)

    def _is_file_different(self, filename, data):
        """Return True if the contents of the file 'filename' differ
        from 'data'. Otherwise return False."""
        try:
            orig_data = json.load(open(filename))
            return orig_data != data
        except IOError, e:
            if e.errno != errno.ENOENT:
                sys.stderr.write('Error reading %s: %s\n' % (filename, e))
                sys.exit(1)
            return True

    def _write_json_if_changed(self, filename, data):
        """Write the given data to the specified filename, but only if it's
        different from what is currently there. Return true if the file was
        written.

        We use this function mostly to avoid changing the timestamps on
        metadata files."""
        if not self._is_file_different(filename, data):
            # Data has not changed--do nothing.
            return False

        f = open(self.tmp_filename, 'w')
        json.dump(data, f)
        f.close()
        os.rename(self.tmp_filename, filename)
        return True

    def _delete_unknown_files(self, rootdir, known, knowntype):
        """Delete all files and directories in rootdir except the
        known files.  knowntype if only used for the log message.
        Returns the number of deleted entries."""
        delete_count = 0
        curr_entries = os.listdir(rootdir)

        unknown_entries = set(curr_entries) - set(known)
        for unknown_entry in unknown_entries:
            fullname = os.path.join(rootdir, unknown_entry)
            self._progress('Deleting unknown %s: %s' % (knowntype, unknown_entry))
            delete_count += 1

            try:
                if os.path.isdir(fullname):
                    shutil.rmtree(fullname)
                else:
                    os.remove(fullname)
            except OSError, e:
                sys.stderr.write('Error deleting %s: %s\n' % (unknown_filename, e.strerror))
                sys.exit(1)

        return delete_count

    def _verbose(self, msg):
        if self.verbosity >= 2:
            print msg

    def _progress(self, msg):
        if self.verbosity >= 1:
            print msg

    def _cleanup(self):
        # Remove a temp file, if one exists
        try:
            os.remove(self.tmp_filename)
        except OSError, e:
            if e.errno != errno.ENOENT:
                sys.stderr.write('Error deleting temp file %s: %s\n' % (self.tmp_filename, e.strerror))

if __name__ == '__main__':
    try:
        parser = argparse.ArgumentParser(
            description='Create a local mirror of your flickr data.')

        parser.add_argument(
            'destdir',
            help='the path to where the mirror shall be stored')

        parser.add_argument(
            '-v', '--verbose',
            dest='verbosity', action="store_const", const=2,
            default=1,
            help='print progress information to stdout')

        parser.add_argument(
            '-q', '--quiet',
            dest='verbosity', action="store_const", const=0,
            help='print nothing to stdout if the mirror succeeds')

        parser.add_argument(
            '-s', '--statistics', action="store_const",
            default=False, const=True,
            help='print transfer-statistics at the end')

        parser.add_argument(
            '--ignore-views', action="store_const",
            dest='include_views', default=True, const=False,
            help="do not include views-counter in metadata" )

        args = parser.parse_args()

        mirrorer = FlickrMirrorer(args.destdir, args.verbosity,
                                  args.statistics, args.include_views)
        mirrorer.run()
    except KeyboardInterrupt:
        # User exited with CTRL+C
        # Print a newline to leave the console in a prettier state
        print
